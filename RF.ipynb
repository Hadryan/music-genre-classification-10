{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RF.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhsangwan/music-genre-classification/blob/main/RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrsZ6HFd92c"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmxP_zWPeSj9"
      },
      "source": [
        "%cd drive/MyDrive/IDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubbO93Iselw6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import plotly.figure_factory as ff\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "import plotly.figure_factory as ff\n",
        "from sklearn.feature_selection import SelectKBest, f_classif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxReuRNeaTJ"
      },
      "source": [
        "data = pd.read_csv(\"music-genre-classification/data/merged_data.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx41TFJ-NCt0"
      },
      "source": [
        "data= data.drop(columns=['Unnamed: 0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn4Q3XqPoLVI"
      },
      "source": [
        "data.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duZX9RN1NYM_"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM4nPa7Aexkt"
      },
      "source": [
        "#explore data\n",
        "print(f'Number of variables - {data.shape[1]}\\n')\n",
        "print(f'Data Types for each variable - \\n{data.dtypes}\\n')\n",
        "print(f'Number of variables for each data type - \\n{data.dtypes.value_counts()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMvcRkVFfFA4"
      },
      "source": [
        "#search missing data\n",
        "#Counting Null values for each column\n",
        "null_count = data.isnull().sum()\n",
        "null_count = null_count[null_count>0]\n",
        "\n",
        "#Printing null counts for columns that have null count >0\n",
        "print(f'Null count for Columns - \\n{null_count}\\n\\n')\n",
        "\n",
        "#Printing % of null counts for columns that have null count>0\n",
        "for column, null_co in null_count.iteritems():\n",
        "    print(f'Percentage of missing values for Column {column} - {null_co*100/data.shape[0]:.2f} %')\n",
        "    if (null_co*100/data.shape[0]) > 0:\n",
        "      col = column \n",
        "      data[data.isnull().any(axis=1)][[col]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq2ddqMyflEc"
      },
      "source": [
        "data = data.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXbEbvJbfqSU"
      },
      "source": [
        "#Counting Null values for each column\n",
        "null_count = data.isnull().sum()\n",
        "null_count = null_count[null_count>0]\n",
        "print(null_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcO567fofvdv"
      },
      "source": [
        "#Checking boolean False counts for each column, an empty/blank string or integer 0 will result in a False value\n",
        "#for boolean type\n",
        "bool_counts = data.astype(bool).sum(axis=0)\n",
        "for col, val in bool_counts.iteritems():\n",
        "    print(f'{col} Percentage Boolean False Counts {(data.shape[0]-val)*100/data.shape[0]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULVSeNHUf50u"
      },
      "source": [
        "sns.countplot(x = 'genre', data = data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqccOc9n-8z"
      },
      "source": [
        "Partition data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv7WXRIUmKdY"
      },
      "source": [
        "data.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4WsRORPyS2F"
      },
      "source": [
        "data_new = data.drop(columns=['genre','title', 'track_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtaUn82EKh6c"
      },
      "source": [
        "#classification data splitting \n",
        "X_train,X_test, Y_train, Y_test = train_test_split(data_new.iloc[:,:], data['genre'], test_size = 0.2, random_state=1)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyY3ap2QK_4r"
      },
      "source": [
        "print(X_train.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWmeNloqLHjU"
      },
      "source": [
        "print(X_val.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-jawelNLufd"
      },
      "source": [
        "# Explore test set\n",
        "print(X_test.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SVLEBoRLRrt"
      },
      "source": [
        "**Standardize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "argX11ioLXuW"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXhx5PJTysp5"
      },
      "source": [
        "#SelectKBest(f_classif, k=20).fit_transform(X_train, Y_train)\n",
        "f_classif(X_train, Y_train)\n",
        "np.mean(f_classif(X_train, Y_train)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQEC1ZSBNMAi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5QDXhXNMD9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf=RandomForestClassifier(n_estimators=200)\n",
        "clf.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OAyYdDNvLC"
      },
      "source": [
        "Y_pred=clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZgDkT4UNxrN"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSEj3U-PZYxy"
      },
      "source": [
        "clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZry7Y5jYqr"
      },
      "source": [
        "conf_matrix = metrics.confusion_matrix(Y_test,Y_pred)\n",
        "sns.heatmap(conf_matrix, annot = True, fmt = \".2f\", square = True, cmap = plt.cm.Blues)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion matrix')\n",
        "plt.tight_layout()\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB88dw6WZc7I"
      },
      "source": [
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "error = 1 - accuracy\n",
        "precision = metrics.precision_score(Y_test, Y_pred, average = None)\n",
        "recall = metrics.recall_score(Y_test, Y_pred, average = None)\n",
        "F1_score = metrics.f1_score(Y_test, Y_pred, average = None)\n",
        "print(\"Accuracy\",accuracy,\"\\nError\", error,\"\\nPrecision\", precision,\"\\nRecall\", recall,\"\\nF1_score\", F1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTHdk4GvPgYN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdu_U2j_v2Y"
      },
      "source": [
        "**Top features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVr9NA18X8hI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Build a classification task using 3 informative features\n",
        "# X, y = make_classification(n_samples=1000,\n",
        "#                            n_features=10,\n",
        "#                            n_informative=3,\n",
        "#                            n_redundant=0,\n",
        "#                            n_repeated=0,\n",
        "#                            n_classes=2,\n",
        "#                            random_state=0,\n",
        "#                            shuffle=False)\n",
        "\n",
        "# Build a forest and compute the impurity-based feature importances\n",
        "forest = clf=RandomForestClassifier(n_estimators=250,\n",
        "                              random_state=0)\n",
        "\n",
        "forest.fit(X_train, Y_train)\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the impurity-based feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices],\n",
        "        color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), indices)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEnV1DbAgT4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDkHw0ZVBHiw"
      },
      "source": [
        "**Cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTSqCm0u0ScI"
      },
      "source": [
        "cv_scores_dict = {}\n",
        "criterion = ['gini', 'entropy']\n",
        "for n_estimators in [100, 200, 300]:\n",
        "    cv_scores = []\n",
        "    for c in C_values:    \n",
        "        RM_model = RandomForestClassifier(n_estimators = n_estimators, criterion=criterion)\n",
        "        cv_scores.append(np.mean(cross_val_score(RM_model, X_train, Y_train, cv=10,n_jobs=-1)))\n",
        "    max_cv_score = max(cv_scores)\n",
        "    max_C_Value = C_values[cv_scores.index(max_cv_score)]\n",
        "    cv_scores_dict[n_estimators] = {'C': max_C_Value, 'cv_score': max_cv_score, 'degree': degree}\n",
        "print(cv_scores_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AFb2ROjwmUn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grWVQZw9wmYR"
      },
      "source": [
        "#modules for hyperparameter optimization\n",
        "\n",
        "print(__doc__)\n",
        "import numpy as np\n",
        "from time import time\n",
        "from operator import itemgetter\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# build a classifier\n",
        "clf = RandomForestClassifier(n_estimators=20)\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(grid_scores, n_top=3):\n",
        "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
        "    for i, score in enumerate(top_scores):\n",
        "        print(\"Model with rank: {0}\".format(i + 1))\n",
        "\n",
        "#since the score is null \"Nan\" is treated as string and hence error below\n",
        "\n",
        "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "              score.mean_validation_score,\n",
        "              np.std(score.cv_validation_scores)))\n",
        "        print(\"Parameters: {0}\".format(score.parameters))\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters space\n",
        "param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": sp_randint(1, 11),\n",
        "              \"min_samples_split\": sp_randint(2, 11),\n",
        "              \"min_samples_leaf\": sp_randint(2, 11),\n",
        "              \"bootstrap\": [True, False],\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X_train, Y_train)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "#print(random.parameters)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}